{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Пример использования Pytorch в задаче классификации изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T15:31:48.625922Z",
     "start_time": "2019-10-25T15:31:47.837997Z"
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab inline\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T15:31:48.955814Z",
     "start_time": "2019-10-25T15:31:48.936719Z"
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "## загрузите датасет https://drive.google.com/open?id=0BxYys69jI14kYVM3aVhKS1VhRUk \n",
    "\n",
    "pdata = Path('../../../UTKFace/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T15:31:49.497664Z",
     "start_time": "2019-10-25T15:31:49.472386Z"
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "fns = list(pdata.glob('*.jpg'))\n",
    "fns = [fn for fn in fns if len(str(fn).split('/')[-1].split('_'))==4 and '__' not in str(fn)]\n",
    "print(len(fns))\n",
    "fns[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T15:31:50.274618Z",
     "start_time": "2019-10-25T15:31:50.185808Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def show_img(fn):\n",
    "    plt.figure()\n",
    "    img = cv2.imread(str(fn))[:,:,::-1]\n",
    "    plt.imshow(img)\n",
    "    \n",
    "for _ in range(2): show_img(np.random.choice(fns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "i2fn = fns\n",
    "fn2i = {fn:i for i,fn in enumerate(i2fn)}\n",
    "bs_fns = [fn.parts[-1] for fn in fns]\n",
    "bs_fns[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "i2age, i2gender, i2race = zip(*[bs_fn.split('_')[:3] for bs_fn in bs_fns])\n",
    "i2age = np.array(i2age, dtype=np.float32)\n",
    "i2gender = np.array(i2gender, dtype=np.int64)\n",
    "i2race = np.array(i2race, dtype=np.int64)\n",
    "\n",
    "o2gender = {0: 'male', 1: 'female'}\n",
    "o2race = dict(list(enumerate(('White', 'Black', 'Asian', 'Indian', 'Others'))))\n",
    "\n",
    "i2race_verbose = [o2race[int(o)] for o in i2race]\n",
    "i2gender_verbose = [o2gender[int(o)] for o in i2gender]\n",
    "print(Counter(i2gender_verbose))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "Counter(i2race_verbose).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sorted(\n",
    "    Counter(list(zip(i2race_verbose, i2gender_verbose))).items(),\n",
    "    key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def show_random_img():\n",
    "    fn = np.random.choice(fns)\n",
    "    i = fn2i[fn]\n",
    "    show_img(fn)\n",
    "    plt.title(f'age: {i2age[i]}; gender: {i2gender_verbose[i]}; race: {i2race_verbose[i]}')\n",
    "    \n",
    "for _ in range(5): show_random_img()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'img_name': bs_fns,\n",
    "    'age': i2age,\n",
    "    'gender': i2gender,\n",
    "    'race': i2race})\n",
    "df['is_train'] = np.random.choice(2, size=len(df), p=[0.2, 0.8])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Обучение моделей\n",
    "\n",
    "## Классификация пола"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from pathlib import PosixPath\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "\n",
    "from dataloaders import ImagesDataset\n",
    "from model import ResnetModel\n",
    "from train_utils import train, validate\n",
    "from train_utils import AccuracyMetric, AccuracyPart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Предобученные модели и эмбединги вы можете скачать [отсюда](https://drive.google.com/open?id=1LcaIDe0AIWe_MzS2BBHxtGKUy0F8J73P)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "args = namedtuple('args', [])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "args.device = device\n",
    "args.print_freq = 1\n",
    "args.batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('img2targets.csv')\n",
    "df[\"img_name\"] = df[\"img_name\"].apply(lambda x: pdata / x)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Определение даталоадеров\n",
    "\n",
    "Нам необходимо два загрузчика данных — для тренировки и для тестирования. \n",
    "\n",
    "dataset[i] должен возвращать преобразованную в тензор картинку и соответствующие лейблы (их может быть несколько)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_data(classes, transforms):\n",
    "    datasets = {\n",
    "        x: ImagesDataset(\n",
    "            df=df, image_paths_name=\"img_name\", labels_names=classes, \n",
    "            is_train=x == \"train\", transform=transforms)\n",
    "        for x in [\"train\", \"dev\"]}\n",
    "\n",
    "    dataloaders = {\n",
    "        x: DataLoader(\n",
    "            dataset=datasets[x], batch_size=args.batch_size, \n",
    "            shuffle=True, num_workers=0, pin_memory=False, \n",
    "            drop_last=True) \n",
    "        for x in [\"train\", \"dev\"]}\n",
    "    \n",
    "    return datasets, dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "datasets, dataloaders = get_data(classes=[\"gender\"], transforms=transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Модель, лосс, оптимизатор\n",
    "\n",
    "Для классификации мы будем использовать предобученный Resnet18. После feature extraction добавим два полносвязных слоя с дропаутом и релу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResnetModel(2, dp=0.5).to(device)\n",
    "model.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# в качестве лосса -- стандартная кросс энтропия\n",
    "criterion = nn.modules.loss.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=1e-3)\n",
    "\n",
    "# = через три эпохи уменьшить learning rate в 10 раз\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer=optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# метрика за которой хотим \"следить\" во время обучения\n",
    "metrics = [AccuracyMetric()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer,\n",
    "                scheduler, metrics, epochs=5):\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        scheduler.step()\n",
    "        train(dataloaders[\"train\"], model, criterion, optimizer, epoch, args, metrics=metrics)\n",
    "        validate(dataloaders[\"dev\"], model, criterion, args, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "## если у вас cpu, то рекомендуетс пропустить блок с обучением и загрузить обученную модель\n",
    "model.load_state_dict(torch.load(\"gender_model.pth\", map_location=device))\n",
    "validate(dataloaders[\"dev\"], model, criterion, args, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# обучение лучше производить на gpu. для инференса на cpu воспользуйтесь загрузкой модели \n",
    "train_model(model, dataloaders, criterion, optimizer, scheduler, metrics, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"gender_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Классификация расы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets, dataloaders = get_data(classes=[\"race\"], transforms=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# меняем только количество классов\n",
    "model = ResnetModel(5, dp=0.5).to(device)\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer=optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "## если у вас cpu, то рекомендуется пропустить блок с обучением и загрузить обученную модель\n",
    "# model.load_state_dict(torch.load(\"race_model.pth\", map_location=device))\n",
    "# validate(dataloaders[\"dev\"], model, criterion, args, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_model(model, dataloaders, criterion, optimizer, scheduler, metrics, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"race_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets, dataloaders = get_data(classes=[\"race\", \"gender\"], transforms=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# финальный лосс -- сумма лоссов по каждой из задач\n",
    "def multi_task_loss(input, targets):\n",
    "    race_loss = nn.functional.cross_entropy(input[:, :5], targets[:, 0])\n",
    "    gender_loss = nn.functional.cross_entropy(input[:, 5:], targets[:, 1])\n",
    "    return race_loss + gender_loss\n",
    "\n",
    "metrics = [AccuracyPart(name=\"RaceAcc\", output_slice=slice(0, 5), target_column=0),\n",
    "           AccuracyPart(name=\"GenderAcc\", output_slice=slice(5, 7), target_column=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# меняем количество классов и лосс\n",
    "model = ResnetModel(7, dp=0.5).to(device)\n",
    "criterion = multi_task_loss\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "## если у вас cpu, то рекомендуется пропустить блок с обучением и загрузить обученную модель\n",
    "# model.load_state_dict(torch.load(\"race_and_gender_model.pth\", map_location=device))\n",
    "# validate(dataloaders[\"dev\"], model, criterion, args, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_model(model, dataloaders, criterion, optimizer, scheduler, metrics, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"race_and_gender_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "def try_multi_on_img(dataset, idx):\n",
    "    tensor, label = dataset[idx]\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        prediction = model(tensor.unsqueeze(0)).detach().cpu().numpy()[0]\n",
    "        race = np.argmax(prediction[:5])\n",
    "        gender = np.argmax(prediction[5:])\n",
    "        show_img(dataset.image_paths[idx])\n",
    "        plt.title(f\"Real: race - {o2race[label[0]]},  gender - {o2gender[label[1]]}. \\n \\\n",
    "        Predicted: race - {o2race[race]}, gender - {o2gender[gender]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = ResnetModel(7, dp=0.5).to(device)\n",
    "state_dict = torch.load(\"race_and_gender_model.pth\", map_location=\"cpu\")\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "for _ in range(4): try_multi_on_img(datasets[\"dev\"], np.random.randint(len(datasets[\"dev\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Сравнение эмбеддингов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как выглядят признаки перед последним слоем для разных моделей. Размерность таких признаков — 512.\n",
    "\n",
    "Для этого воспользуемся функцией get_embedding() нашего классификатора. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "def compute_embeddings(model, dataloader):\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i, (data, labels) in tqdm(enumerate(dataloader)):\n",
    "            all_labels.append(labels.numpy())\n",
    "            data = data.to(device)\n",
    "            embeddings = model.get_embedding(data).detach().cpu().numpy()\n",
    "            all_embeddings.append(embeddings)\n",
    "        \n",
    "    return all_embeddings, all_labels\n",
    "\n",
    "def save_for_projector(embeddings, metadata, name):\n",
    "    embeddings = pd.DataFrame(np.concatenate(embeddings))\n",
    "    metadata = pd.DataFrame(np.concatenate(metadata), columns=[\"Race\", \"Gender\"])\n",
    "    embeddings.to_csv(f\"embeddings_{name}.tsv\", header=None, index=None, sep=\"\\t\")\n",
    "    metadata.to_csv(f\"metadata_{name}.tsv\", index=None, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "model = ResnetModel(7, dp=0.5).to(device)\n",
    "state_dict = torch.load(\"race_and_gender_model.pth\", map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "embeddings, metadata = compute_embeddings(model, dataloaders[\"dev\"])\n",
    "save_for_projector(embeddings, metadata, \"race&gender\")\n",
    "\n",
    "model = ResnetModel(2, dp=0.5).to(device)\n",
    "state_dict = torch.load(\"gender_model.pth\", map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "embeddings, metadata = compute_embeddings(model, dataloaders[\"dev\"])\n",
    "save_for_projector(embeddings, metadata, \"gender_only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Загрузив полученные файлы в https://projector.tensorflow.org/ получаем ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для гендерной модели, отчетливо выделяется кластера — male, female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "W = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"images/gender_only_gender.PNG\", width=W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "При этом расы расположены \"в перемешку\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"images/gender_only_race.PNG\", width=W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Модель, отвечающая сразу за две задачи, выучила соответствующие признаки, в пространстве которых объекты близки как по полу так и по расе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"images/gender&race_gender.PNG\", width=W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Image(filename=\"images/gender&race_race.PNG\", width=W)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "dmia2019",
   "language": "python",
   "name": "dmia2019"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
